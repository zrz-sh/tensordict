
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/tensordict_slicing.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorials_tensordict_slicing.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_tensordict_slicing.py:


Slicing, Indexing, and Masking
==============================
**Author**: `Tom Begley <https://github.com/tcbegley>`_

In this tutorial you will learn how to slice, index, and mask a :class:`~.TensorDict`.

.. GENERATED FROM PYTHON SOURCE LINES 11-24

As discussed in the tutorial
`Manipulating the shape of a TensorDict <./tensordict_shapes.html>`_, when we create a
:class:`~.TensorDict` we specify a ``batch_size``, which must agree
with the leading dimensions of all entries in the :class:`~.TensorDict`. Since we have
a guarantee that all entries share those dimensions in common, we are able to index
and mask the batch dimensions in the same way that we would index a
:class:`torch.Tensor`. The indices are applied along the batch dimensions to all of
the entries in the :class:`~.TensorDict`.

For example, given a :class:`~.TensorDict` with two batch dimensions,
``tensordict[0]`` returns a new :class:`~.TensorDict` with the same structure, and
whose values correspond to the first "row" of each entry in the original
:class:`~.TensorDict`.

.. GENERATED FROM PYTHON SOURCE LINES 24-34

.. code-block:: Python


    import torch
    from tensordict import TensorDict

    tensordict = TensorDict(
        {"a": torch.zeros(3, 4, 5), "b": torch.zeros(3, 4)}, batch_size=[3, 4]
    )

    print(tensordict[0])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(shape=torch.Size([4, 5]), device=cpu, dtype=torch.float32, is_shared=False),
            b: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([4]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 35-37

The same syntax applies as for regular tensors. For example if we wanted to drop the
first row of each entry we could index as follows

.. GENERATED FROM PYTHON SOURCE LINES 37-40

.. code-block:: Python


    print(tensordict[1:])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(shape=torch.Size([2, 4, 5]), device=cpu, dtype=torch.float32, is_shared=False),
            b: Tensor(shape=torch.Size([2, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([2, 4]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 41-42

We can index multiple dimensions simultaneously

.. GENERATED FROM PYTHON SOURCE LINES 42-45

.. code-block:: Python


    print(tensordict[:, 2:])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),
            b: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([3, 2]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 46-48

We can also use ``Ellipsis`` to represent as many ``:`` as would be needed to make
the selection tuple the same length as ``tensordict.batch_dims``.

.. GENERATED FROM PYTHON SOURCE LINES 48-51

.. code-block:: Python


    print(tensordict[..., 2:])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            a: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),
            b: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([3, 2]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 52-65

.. note:

   Remember that all indexing is applied relative to the batch dimensions. In the
   above example there is a difference between ``tensordict["a"][..., 2:]`` and
   ``tensordict[..., 2:]["a"]``. The first retrieves the three-dimensional tensor
   stored under the key ``"a"`` and applies the index ``2:`` to the final dimension.
   The second applies the index ``2:`` to the final *batch dimension*, which is the
   second dimension, before retrieving the result.

Setting Values with Indexing
----------------------------
In general, ``tensordict[index] = new_tensordict`` will work as long as the batch
sizes are compatible.

.. GENERATED FROM PYTHON SOURCE LINES 65-74

.. code-block:: Python


    tensordict = TensorDict(
        {"a": torch.zeros(3, 4, 5), "b": torch.zeros(3, 4)}, batch_size=[3, 4]
    )

    td2 = TensorDict({"a": torch.ones(2, 4, 5), "b": torch.ones(2, 4)}, batch_size=[2, 4])
    tensordict[:-1] = td2
    print(tensordict["a"], tensordict["b"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[[1., 1., 1., 1., 1.],
             [1., 1., 1., 1., 1.],
             [1., 1., 1., 1., 1.],
             [1., 1., 1., 1., 1.]],

            [[1., 1., 1., 1., 1.],
             [1., 1., 1., 1., 1.],
             [1., 1., 1., 1., 1.],
             [1., 1., 1., 1., 1.]],

            [[0., 0., 0., 0., 0.],
             [0., 0., 0., 0., 0.],
             [0., 0., 0., 0., 0.],
             [0., 0., 0., 0., 0.]]]) tensor([[1., 1., 1., 1.],
            [1., 1., 1., 1.],
            [0., 0., 0., 0.]])




.. GENERATED FROM PYTHON SOURCE LINES 75-78

Masking
-------
We mask :class:`TensorDict` as we mask tensors.

.. GENERATED FROM PYTHON SOURCE LINES 78-82

.. code-block:: Python


    mask = torch.BoolTensor([[1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0]])
    tensordict[mask]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            a: Tensor(shape=torch.Size([6, 5]), device=cpu, dtype=torch.float32, is_shared=False),
            b: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([6]),
        device=None,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 83-89

SubTensorDict
-------------
When we index a :class:`~.TensorDict` with a contiguous index, we obtain a new
:class:`~.TensorDict` whose values are all views on the values of the original
:class:`~.TensorDict`. That means updates to the indexed :class:`~.TensorDict` are
applied to the original also.

.. GENERATED FROM PYTHON SOURCE LINES 89-99

.. code-block:: Python


    tensordict = TensorDict(
        {"a": torch.zeros(3, 4, 5), "b": torch.zeros(3, 4)}, batch_size=[3, 4]
    )
    td2 = tensordict[1:]
    td2.fill_("b", 1)

    assert (tensordict["b"][1:] == 1).all()
    print(tensordict["b"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[0., 0., 0., 0.],
            [1., 1., 1., 1.],
            [1., 1., 1., 1.]])




.. GENERATED FROM PYTHON SOURCE LINES 100-101

This doesn't work however if we use a non-contiguous index

.. GENERATED FROM PYTHON SOURCE LINES 101-111

.. code-block:: Python


    tensordict = TensorDict(
        {"a": torch.zeros(3, 4, 5), "b": torch.zeros(3, 4)}, batch_size=[3, 4]
    )
    td2 = tensordict[[0, 2]]
    td2.fill_("b", 1)

    assert (tensordict == 0).all()
    print(tensordict["b"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[0., 0., 0., 0.],
            [0., 0., 0., 0.],
            [0., 0., 0., 0.]])




.. GENERATED FROM PYTHON SOURCE LINES 112-117

In case such functionality is needed, one can use
:meth:`TensorDict.get_sub_tensordict <tensordict.TensorDict.get_sub_tensordict>`
instead. The :class:`~.SubTensorDict` holds a reference to the orgiinal
:class:`~.TensorDict` so that updates to the sub-tensordict can be written back to the
source.

.. GENERATED FROM PYTHON SOURCE LINES 117-124

.. code-block:: Python


    tensordict = TensorDict(
        {"a": torch.zeros(3, 4, 5), "b": torch.zeros(3, 4)}, batch_size=[3, 4]
    )
    td2 = tensordict.get_sub_tensordict(([0, 2],))
    td2.fill_("b", 1)
    print(tensordict["b"])




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[1., 1., 1., 1.],
            [0., 0., 0., 0.],
            [1., 1., 1., 1.]])





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.007 seconds)


.. _sphx_glr_download_tutorials_tensordict_slicing.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tensordict_slicing.ipynb <tensordict_slicing.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tensordict_slicing.py <tensordict_slicing.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
