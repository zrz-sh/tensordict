{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Manipulating the shape of a TensorDict\n**Author**: [Tom Begley](https://github.com/tcbegley)\n\nIn this tutorial you will learn how to manipulate the shape of a :class:`~.TensorDict`\nand its contents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we create a :class:`~.TensorDict` we specify a ``batch_size``, which must agree\nwith the leading dimensions of all entries in the :class:`~.TensorDict`. Since we have\na guarantee that all entries share those dimensions in common, :class:`~.TensorDict`\nis able to expose a number of methods with which we can manipulate the shape of the\n:class:`~.TensorDict` and its contents.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom tensordict.tensordict import TensorDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Indexing a ``TensorDict``\n\nSince the batch dimensions are guaranteed to exist on all entries, we can index them\nas we please, and each entry of the :class:`~.TensorDict` will be indexed in the same\nway.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.rand(3, 4)\nb = torch.rand(3, 4, 5)\ntensordict = TensorDict({\"a\": a, \"b\": b}, batch_size=[3, 4])\n\nindexed_tensordict = tensordict[:2, 1]\nassert indexed_tensordict[\"a\"].shape == torch.Size([2])\nassert indexed_tensordict[\"b\"].shape == torch.Size([2, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reshaping a ``TensorDict``\n\n:meth:`TensorDict.reshape <tensordict.TensorDict.reshape>` works just like\n:meth:`torch.Tensor.reshape`. It applies to all of the contents of the\n:class:`~.TensorDict` along the batch dimensions - note the shape of ``b`` in the\nexample below. It also updates the ``batch_size`` attribute.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reshaped_tensordict = tensordict.reshape(-1)\nassert reshaped_tensordict.batch_size == torch.Size([12])\nassert reshaped_tensordict[\"a\"].shape == torch.Size([12])\nassert reshaped_tensordict[\"b\"].shape == torch.Size([12, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting a ``TensorDict``\n\n:meth:`TensorDict.split <tensordict.TensorDict.split>` is similar to\n:meth:`torch.Tensor.split`. It splits the :class:`~.TensorDict` into chunks. Each\nchunk is a :class:`~.TensorDict` with the same structure as the original one, but\nwhose entries are views of the corresponding entries in the original\n:class:`~.TensorDict`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "chunks = tensordict.split([3, 1], dim=1)\nassert chunks[0].batch_size == torch.Size([3, 3])\nassert chunks[1].batch_size == torch.Size([3, 1])\ntorch.testing.assert_allclose(chunks[0][\"a\"], tensordict[\"a\"][:, :-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Whenever a function or method accepts a ``dim`` argument, negative dimensions are\n   interpreted relative to the ``batch_size`` of the :class:`~.TensorDict` that the\n   function or method is called on. In particular, if there are nested\n   :class:`~.TensorDict` values with different batch sizes, the negative dimension is\n   always interpreted relative to the batch dimensions of the root.\n\n```\ntensordict = TensorDict(\n    {\n        \"a\": torch.rand(3, 4),\n        \"nested\": TensorDict({\"b\": torch.rand(3, 4, 5)}, [3, 4, 5])\n    },\n    [3, 4],\n)\n# dim = -2 will be interpreted as the first dimension throughout, as the root\n# TensorDict has 2 batch dimensions, even though the nested TensorDict has 3\nchunks = tensordict.split([2, 1], dim=-2)\nassert chunks[0].batch_size == torch.Size([2, 4])\nassert chunks[0][\"nested\"].batch_size == torch.Size([2, 4, 5])\n```\n   As you can see from this example, the\n   :meth:`TensorDict.split <tensordict.TensorDict.split>` method behaves exactly as\n   though we had replaced ``dim=-2`` with ``dim=tensordict.batch_dims - 2`` before\n   calling.</p></div>\n\n## Unbind\n:meth:`TensorDict.unbind <tensordict.TensorDict.unbind>` is similar to\n:meth:`torch.Tensor.unbind`, and conceptually similar to\n:meth:`TensorDict.split <tensordict.TensorDict.split>`. It removes the specified\ndimension and returns a ``tuple`` of all slices along that dimension.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "slices = tensordict.unbind(dim=1)\nassert len(slices) == 4\nassert all(s.batch_size == torch.Size([3]) for s in slices)\ntorch.testing.assert_allclose(slices[0][\"a\"], tensordict[\"a\"][:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacking and concatenating\n\n:class:`~.TensorDict` can be used in conjunction with ``torch.cat`` and ``torch.stack``.\n\n### Stacking ``TensorDict``\nBy default, stacking is done in a lazy fashion, returning a\n:class:`LazyStackedTensorDict` object. In this case values are only stacked on-demand\nwhen they are accessed. This in cases where you have a large :class:`~.TensorDict` with\nmany entries, and you don't need to stack all of them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cloned_tensordict = tensordict.clone()\n# no stacking happens on the next line\nstacked_tensordict = torch.stack([tensordict, cloned_tensordict], dim=0)\nprint(stacked_tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we index a :class:`~.LazyStackedTensorDict` along the stacking dimension we recover\nthe original :class:`~.TensorDict`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert stacked_tensordict[0] is tensordict\nassert stacked_tensordict[1] is cloned_tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing a key in the :class:`~.LazyStackedTensorDict` results in those values being\nstacked. If the key corresponds to a nested :class:`~.TensorDict` then we will recover\nanother :class:`~.LazyStackedTensorDict`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert stacked_tensordict[\"a\"].shape == torch.Size([2, 3, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Since values are stacked on-demand, accessing an item multiple times will mean it\n   gets stacked multiple times, which is inefficient. If you need to access a value\n   in the stacked :class:`~.TensorDict` more than once, you may want to consider\n   converting the :class:`LazyStackedTensorDict` to a contiguous\n   :class:`~.TensorDict`, which can be done with the\n   :meth:`LazyStackedTensorDict.to_tensordict <tensordict.LazyStackedTensorDict.to_tensordict>`\n   or :meth:`LazyStackedTensorDict.contiguous <tensordict.LazyStackedTensorDict.contiguous>`\n   methods.\n\n   .. code-block::\n      assert isinstance(stacked_tensordict.contiguous(), TensorDict)\n      assert isinstance(stacked_tensordict.contiguous(), TensorDict)\n\n   After calling either of these methods, we will have a regular :class:`TensorDict`\n   containing the stacked values, and no additional computation is performed when\n   values are accessed.</p></div>\n\n### Concatenating ``TensorDict``\nConcatenation is not done lazily, instead calling :func:`torch.cat` on a list of\n:class:`~.TensorDict` instances simply returns a :class:`~.TensorDict` whose entries\nare the concatenated entries of the elements of the list.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "concatenated_tensordict = torch.cat([tensordict, cloned_tensordict], dim=0)\nassert isinstance(concatenated_tensordict, TensorDict)\nassert concatenated_tensordict.batch_size == torch.Size([6, 4])\nassert concatenated_tensordict[\"b\"].shape == torch.Size([6, 4, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expanding ``TensorDict``\nWe can expand all of the entries of a :class:`~.TensorDict` using\n:meth:`TensorDict.expand <tensordict.TensorDict.expand>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exp_tensordict = tensordict.expand(2, *tensordict.batch_size)\nassert exp_tensordict.batch_size == torch.Size([2, 3, 4])\ntorch.testing.assert_allclose(exp_tensordict[\"a\"][0], exp_tensordict[\"a\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Squeezing and Unsqueezing ``TensorDict``\nWe can squeeze or unsqueeze the contents of a :class:`~.TensorDict` with the\n:meth:`TensorDict.squeeze <tensordict.TensorDict.squeeze>` and\n:meth:`TensorDict.unsqueeze <tensordict.TensorDict.unsqueeze>` methods. These\noperations are both lazy, and return\n:class:`_SqueezedTensorDict <tensordict.tensordict._SqueezedTensorDict>` and\n:class:`_UnsqueezedTensorDict <tensordict.tensordict._UnsqueezedTensorDict>`\nrespectively. Like in the case of stacking and :class:`~.LazyStackedTensorDict`, the\nsqueezing or unsqueezing is only performed when we try to access an entry.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(3, 1, 4)}, [3, 1, 4])\nsqueezed_tensordict = tensordict.squeeze()\nassert squeezed_tensordict[\"a\"].shape == torch.Size([3, 4])\nprint(squeezed_tensordict, end=\"\\n\\n\")\n\nunsqueezed_tensordict = tensordict.unsqueeze(-1)\nassert unsqueezed_tensordict[\"a\"].shape == torch.Size([3, 1, 4, 1])\nprint(unsqueezed_tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you are likely to repeated access the same entry in the squeezed or unsqueezed\n   tensordict, then it may be beneficial to first convert to a regular\n   :class:`~.TensorDict` using :meth:`.to_tensordict()`\n\n```\ntensordict = TensorDict({\"a\": torch.rand(3, 1, 4)}, [3, 1, 4])\nsqueezed_tensordict = tensordict.squeeze().to_tensordict()\nassert isinstance(squeezed_tensordict, TensorDict)\nassert squeezed_tensordict.batch_size == torch.Size([3, 1, 4])\n```\n   In this case, ``squeezed_tensordict`` is a regular :class:`TensorDict` containing\n   the squeezed tensors.</p></div>\n\nBear in mind that as ever, these methods apply only to the batch dimensions. Any non\nbatch dimensions of the entries will be unaffected\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(3, 1, 1, 4)}, [3, 1])\nsqueezed_tensordict = tensordict.squeeze()\n# only one of the singleton dimensions is dropped as the other\n# is not a batch dimension\nassert squeezed_tensordict[\"a\"].shape == torch.Size([3, 1, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing a TensorDict\n:class:`~.TensorDict` also supports ``view``. This creates a ``_ViewedTensorDict``\nwhich lazily creates views on its contents when they are accessed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.arange(12)}, [12])\n# no views are created at this step\nviewed_tensordict = tensordict.view((2, 3, 2))\n\n# the view of \"a\" is created on-demand when we access it\nassert viewed_tensordict[\"a\"].shape == torch.Size([2, 3, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Permuting batch dimensions\nThe :meth:`TensorDict.permute <tensordict.TensorDict.permute>` method can be used to\npermute the batch dimensions much like :func:`torch.permute`. Non batch dimensions are\nleft untouched.\n\nThis operation is lazy, so batch dimensions are only permuted when we try to access\nthe entries. As ever, if you are likely to need to access a particular entry multiple\ntimes, consider converting to a :class:`~.TensorDict`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(3, 4), \"b\": torch.rand(3, 4, 5)}, [3, 4])\n# swap the batch dimensions\npermuted_tensordict = tensordict.permute([1, 0])\n\nassert permuted_tensordict[\"a\"].shape == torch.Size([4, 3])\nassert permuted_tensordict[\"b\"].shape == torch.Size([4, 3, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gathering values in ``TensorDict``\nThe :meth:`TensorDict.gather <tensordict.TensorDict.gather>` method can be used to\nindex along the batch dimensions and gather the results into a single dimension much\nlike :func:`torch.gather`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "index = torch.randint(4, (3, 4))\ngathered_tensordict = tensordict.gather(dim=1, index=index)\nprint(\"index:\\n\", index, end=\"\\n\\n\")\nprint(\"tensordict['a']:\\n\", tensordict[\"a\"], end=\"\\n\\n\")\nprint(\"gathered_tensordict['a']:\\n\", gathered_tensordict[\"a\"], end=\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}