{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# TensorDictModule\nIn this tutorial you will learn how to use :class:`~.TensorDictModule` and\n:class:`~.TensorDictSequential` to create generic and reusable modules that can accept\n:class:`~.TensorDict` as input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a convenient usage of the :class:`~.TensorDict` class with ``nn.Module``,\n:mod:`tensordict` provides an interface between the two named ``TensorDictModule``.\nThe ``TensorDictModule`` class is an ``nn.Module`` that takes a\n:class:`~.TensorDict` as input when called.\nIt is up to the user to define the keys to be read as input and output.\n\n## TensorDictModule by examples\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom tensordict import TensorDict\nfrom tensordict.nn import TensorDictModule, TensorDictSequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Simple usage\nWe have a :class:`~.TensorDict` with 2 entries ``\"a\"`` and ``\"b\"`` but only the\nvalue associated with ``\"a\"`` has to be read by the network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    {\"a\": torch.randn(5, 3), \"b\": torch.zeros(5, 4, 3)},\n    batch_size=[5],\n)\nlinear = TensorDictModule(nn.Linear(3, 10), in_keys=[\"a\"], out_keys=[\"a_out\"])\nlinear(tensordict)\nassert (tensordict.get(\"b\") == 0).all()\nprint(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Multiple inputs\nSuppose we have a slightly more complex network that takes 2 entries and\naverages them into a single output tensor. To make a ``TensorDictModule``\ninstance read multiple input values, one must register them in the\n``in_keys`` keyword argument of the constructor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MergeLinear(nn.Module):\n    def __init__(self, in_1, in_2, out):\n        super().__init__()\n        self.linear_1 = nn.Linear(in_1, out)\n        self.linear_2 = nn.Linear(in_2, out)\n\n    def forward(self, x_1, x_2):\n        return (self.linear_1(x_1) + self.linear_2(x_2)) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    {\n        \"a\": torch.randn(5, 3),\n        \"b\": torch.randn(5, 4),\n    },\n    batch_size=[5],\n)\n\nmergelinear = TensorDictModule(\n    MergeLinear(3, 4, 10), in_keys=[\"a\", \"b\"], out_keys=[\"output\"]\n)\n\nmergelinear(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Multiple outputs\nSimilarly, ``TensorDictModule`` not only supports multiple inputs but also\nmultiple outputs. To make a ``TensorDictModule`` instance write to multiple\noutput values, one must register them in the ``out_keys`` keyword argument\nof the constructor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MultiHeadLinear(nn.Module):\n    def __init__(self, in_1, out_1, out_2):\n        super().__init__()\n        self.linear_1 = nn.Linear(in_1, out_1)\n        self.linear_2 = nn.Linear(in_1, out_2)\n\n    def forward(self, x):\n        return self.linear_1(x), self.linear_2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n\nsplitlinear = TensorDictModule(\n    MultiHeadLinear(3, 4, 10),\n    in_keys=[\"a\"],\n    out_keys=[\"output_1\", \"output_2\"],\n)\nsplitlinear(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When having multiple input keys and output keys, make sure they match the\norder in the module.\n\n``TensorDictModule`` can work with :class:`~.TensorDict` instances that contain\nmore tensors than what the ``in_keys`` attribute indicates.\n\nUnless a ``vmap`` operator is used, the :class:`~.TensorDict` is modified in-place.\n\n**Ignoring some outputs**\n\nNote that it is possible to avoid writing some of the tensors to the\n:class:`~.TensorDict` output, using ``\"_\"`` in ``out_keys``.\n\n### Example 4: Combining multiple ``TensorDictModule`` with ``TensorDictSequential``\nTo combine multiple ``TensorDictModule`` instances, we can use\n``TensorDictSequential``. We create a list where each ``TensorDictModule`` must\nbe executed sequentially. ``TensorDictSequential`` will read and write keys to the\ntensordict following the sequence of modules provided.\n\nWe can also gather the inputs needed by ``TensorDictSequential`` with the\n``in_keys`` property, and the outputs keys are found at the ``out_keys`` attribute.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n\nsplitlinear = TensorDictModule(\n    MultiHeadLinear(3, 4, 10),\n    in_keys=[\"a\"],\n    out_keys=[\"output_1\", \"output_2\"],\n)\nmergelinear = TensorDictModule(\n    MergeLinear(4, 10, 13),\n    in_keys=[\"output_1\", \"output_2\"],\n    out_keys=[\"output\"],\n)\n\nsplit_and_merge_linear = TensorDictSequential(splitlinear, mergelinear)\n\nassert split_and_merge_linear(tensordict)[\"output\"].shape == torch.Size([5, 13])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Do's and don't with TensorDictModule\n\nDon't use ``nn.Sequence``, similar to ``nn.Module``, it would break features\nsuch as ``functorch`` compatibility. Do use ``TensorDictSequential`` instead.\n\nDon't assign the output tensordict to a new variable, as the output\ntensordict is just the input modified in-place:\n\n  tensordict = module(tensordict)  # ok!\n\n  tensordict_out = module(tensordict)  # don't!\n\n### ``ProbabilisticTensorDictModule``\n``ProbabilisticTensorDictModule`` is a non-parametric module representing a\nprobability distribution. Distribution parameters are read from tensordict\ninput, and the output is written to an output tensordict. The output is\nsampled given some rule, specified by the input ``default_interaction_type``\nargument and the ``exploration_mode()`` global function. If they conflict,\nthe context manager precedes.\n\nIt can be wired together with a ``TensorDictModule`` that returns\na tensordict updated with the distribution parameters using\n``ProbabilisticTensorDictSequential``. This is a special case of\n``TensorDictSequential`` that terminates in a\n``ProbabilisticTensorDictModule``.\n\n``ProbabilisticTensorDictModule`` is responsible for constructing the\ndistribution (through the ``get_dist()`` method) and/or sampling from this\ndistribution (through a regular ``__call__()`` to the module). The same\n``get_dist()`` method is exposed on ``ProbabilisticTensorDictSequential.\n\nOne can find the parameters in the output tensordict as well as the log\nprobability if needed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import (\n    ProbabilisticTensorDictModule,\n    ProbabilisticTensorDictSequential,\n)\nfrom tensordict.nn.distributions import NormalParamExtractor\nfrom torch import distributions as dist\n\ntd = TensorDict({\"input\": torch.randn(3, 4), \"hidden\": torch.randn(3, 8)}, [3])\nnet = torch.nn.GRUCell(4, 8)\nnet = TensorDictModule(net, in_keys=[\"input\", \"hidden\"], out_keys=[\"hidden\"])\nextractor = NormalParamExtractor()\nextractor = TensorDictModule(extractor, in_keys=[\"hidden\"], out_keys=[\"loc\", \"scale\"])\ntd_module = ProbabilisticTensorDictSequential(\n    net,\n    extractor,\n    ProbabilisticTensorDictModule(\n        in_keys=[\"loc\", \"scale\"],\n        out_keys=[\"action\"],\n        distribution_class=dist.Normal,\n        return_log_prob=True,\n    ),\n)\nprint(f\"TensorDict before going through module: {td}\")\ntd_module(td)\nprint(f\"TensorDict after going through module now as keys action, loc and scale: {td}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Showcase: Implementing a transformer using TensorDictModule\nTo demonstrate the flexibility of ``TensorDictModule``, we are going to\ncreate a transformer that reads :class:`~.TensorDict` objects using ``TensorDictModule``.\n\nThe following figure shows the classical transformer architecture\n(Vaswani et al, 2017).\n\n<img src=\"reference/generated/tutorials/media/reference/generated/tutorials/media/transformer.png\" alt=\"The transformer png\">\n\nWe have let the positional encoders aside for simplicity.\n\nLet's re-write the classical transformers blocks:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class TokensToQKV(nn.Module):\n    def __init__(self, to_dim, from_dim, latent_dim):\n        super().__init__()\n        self.q = nn.Linear(to_dim, latent_dim)\n        self.k = nn.Linear(from_dim, latent_dim)\n        self.v = nn.Linear(from_dim, latent_dim)\n\n    def forward(self, X_to, X_from):\n        Q = self.q(X_to)\n        K = self.k(X_from)\n        V = self.v(X_from)\n        return Q, K, V\n\n\nclass SplitHeads(nn.Module):\n    def __init__(self, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n\n    def forward(self, Q, K, V):\n        batch_size, to_num, latent_dim = Q.shape\n        _, from_num, _ = K.shape\n        d_tensor = latent_dim // self.num_heads\n        Q = Q.reshape(batch_size, to_num, self.num_heads, d_tensor).transpose(1, 2)\n        K = K.reshape(batch_size, from_num, self.num_heads, d_tensor).transpose(1, 2)\n        V = V.reshape(batch_size, from_num, self.num_heads, d_tensor).transpose(1, 2)\n        return Q, K, V\n\n\nclass Attention(nn.Module):\n    def __init__(self, latent_dim, to_dim):\n        super().__init__()\n        self.softmax = nn.Softmax(dim=-1)\n        self.out = nn.Linear(latent_dim, to_dim)\n\n    def forward(self, Q, K, V):\n        batch_size, n_heads, to_num, d_in = Q.shape\n        attn = self.softmax(Q @ K.transpose(2, 3) / d_in)\n        out = attn @ V\n        out = self.out(out.transpose(1, 2).reshape(batch_size, to_num, n_heads * d_in))\n        return out, attn\n\n\nclass SkipLayerNorm(nn.Module):\n    def __init__(self, to_len, to_dim):\n        super().__init__()\n        self.layer_norm = nn.LayerNorm((to_len, to_dim))\n\n    def forward(self, x_0, x_1):\n        return self.layer_norm(x_0 + x_1)\n\n\nclass FFN(nn.Module):\n    def __init__(self, to_dim, hidden_dim, dropout_rate=0.2):\n        super().__init__()\n        self.FFN = nn.Sequential(\n            nn.Linear(to_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, to_dim),\n            nn.Dropout(dropout_rate),\n        )\n\n    def forward(self, X):\n        return self.FFN(X)\n\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, to_dim, to_len, from_dim, latent_dim, num_heads):\n        super().__init__()\n        self.tokens_to_qkv = TokensToQKV(to_dim, from_dim, latent_dim)\n        self.split_heads = SplitHeads(num_heads)\n        self.attention = Attention(latent_dim, to_dim)\n        self.skip = SkipLayerNorm(to_len, to_dim)\n\n    def forward(self, X_to, X_from):\n        Q, K, V = self.tokens_to_qkv(X_to, X_from)\n        Q, K, V = self.split_heads(Q, K, V)\n        out, attention = self.attention(Q, K, V)\n        out = self.skip(X_to, out)\n        return out\n\n\nclass EncoderTransformerBlock(nn.Module):\n    def __init__(self, to_dim, to_len, latent_dim, num_heads):\n        super().__init__()\n        self.attention_block = AttentionBlock(\n            to_dim, to_len, to_dim, latent_dim, num_heads\n        )\n        self.FFN = FFN(to_dim, 4 * to_dim)\n        self.skip = SkipLayerNorm(to_len, to_dim)\n\n    def forward(self, X_to):\n        X_to = self.attention_block(X_to, X_to)\n        X_out = self.FFN(X_to)\n        return self.skip(X_out, X_to)\n\n\nclass DecoderTransformerBlock(nn.Module):\n    def __init__(self, to_dim, to_len, from_dim, latent_dim, num_heads):\n        super().__init__()\n        self.attention_block = AttentionBlock(\n            to_dim, to_len, from_dim, latent_dim, num_heads\n        )\n        self.encoder_block = EncoderTransformerBlock(\n            to_dim, to_len, latent_dim, num_heads\n        )\n\n    def forward(self, X_to, X_from):\n        X_to = self.attention_block(X_to, X_from)\n        X_to = self.encoder_block(X_to)\n        return X_to\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, num_blocks, to_dim, to_len, latent_dim, num_heads):\n        super().__init__()\n        self.encoder = nn.ModuleList(\n            [\n                EncoderTransformerBlock(to_dim, to_len, latent_dim, num_heads)\n                for i in range(num_blocks)\n            ]\n        )\n\n    def forward(self, X_to):\n        for i in range(len(self.encoder)):\n            X_to = self.encoder[i](X_to)\n        return X_to\n\n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, num_blocks, to_dim, to_len, from_dim, latent_dim, num_heads):\n        super().__init__()\n        self.decoder = nn.ModuleList(\n            [\n                DecoderTransformerBlock(to_dim, to_len, from_dim, latent_dim, num_heads)\n                for i in range(num_blocks)\n            ]\n        )\n\n    def forward(self, X_to, X_from):\n        for i in range(len(self.decoder)):\n            X_to = self.decoder[i](X_to, X_from)\n        return X_to\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self, num_blocks, to_dim, to_len, from_dim, from_len, latent_dim, num_heads\n    ):\n        super().__init__()\n        self.encoder = TransformerEncoder(\n            num_blocks, to_dim, to_len, latent_dim, num_heads\n        )\n        self.decoder = TransformerDecoder(\n            num_blocks, from_dim, from_len, to_dim, latent_dim, num_heads\n        )\n\n    def forward(self, X_to, X_from):\n        X_to = self.encoder(X_to)\n        X_out = self.decoder(X_from, X_to)\n        return X_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first create the ``AttentionBlockTensorDict``, the attention block using\n``TensorDictModule`` and ``TensorDictSequential``.\n\nThe wiring operation that connects the modules to each other requires us\nto indicate which key each of them must read and write. Unlike\n``nn.Sequence``, a ``TensorDictSequential`` can read/write more than one\ninput/output. Moreover, its components inputs need not be identical to the\nprevious layers outputs, allowing us to code complicated neural architecture.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AttentionBlockTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            TensorDictModule(\n                TokensToQKV(to_dim, from_dim, latent_dim),\n                in_keys=[to_name, from_name],\n                out_keys=[\"Q\", \"K\", \"V\"],\n            ),\n            TensorDictModule(\n                SplitHeads(num_heads),\n                in_keys=[\"Q\", \"K\", \"V\"],\n                out_keys=[\"Q\", \"K\", \"V\"],\n            ),\n            TensorDictModule(\n                Attention(latent_dim, to_dim),\n                in_keys=[\"Q\", \"K\", \"V\"],\n                out_keys=[\"X_out\", \"Attn\"],\n            ),\n            TensorDictModule(\n                SkipLayerNorm(to_len, to_dim),\n                in_keys=[to_name, \"X_out\"],\n                out_keys=[to_name],\n            ),\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We build the encoder and decoder blocks that will be part of the transformer\nthanks to ``TensorDictModule``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class TransformerBlockEncoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            AttentionBlockTensorDict(\n                to_name,\n                from_name,\n                to_dim,\n                to_len,\n                from_dim,\n                latent_dim,\n                num_heads,\n            ),\n            TensorDictModule(\n                FFN(to_dim, 4 * to_dim),\n                in_keys=[to_name],\n                out_keys=[\"X_out\"],\n            ),\n            TensorDictModule(\n                SkipLayerNorm(to_len, to_dim),\n                in_keys=[to_name, \"X_out\"],\n                out_keys=[to_name],\n            ),\n        )\n\n\nclass TransformerBlockDecoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            AttentionBlockTensorDict(\n                to_name,\n                to_name,\n                to_dim,\n                to_len,\n                to_dim,\n                latent_dim,\n                num_heads,\n            ),\n            TransformerBlockEncoderTensorDict(\n                to_name,\n                from_name,\n                to_dim,\n                to_len,\n                from_dim,\n                latent_dim,\n                num_heads,\n            ),\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the transformer encoder and decoder.\n\nFor an encoder, we just need to take the same tokens for both queries,\nkeys and values.\n\nFor a decoder, we now can extract info from ``X_from`` into ``X_to``.\n``X_from`` will map to queries whereas ``X_from`` will map to keys and values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        num_blocks,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            *[\n                TransformerBlockEncoderTensorDict(\n                    to_name,\n                    from_name,\n                    to_dim,\n                    to_len,\n                    from_dim,\n                    latent_dim,\n                    num_heads,\n                )\n                for _ in range(num_blocks)\n            ]\n        )\n\n\nclass TransformerDecoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        num_blocks,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            *[\n                TransformerBlockDecoderTensorDict(\n                    to_name,\n                    from_name,\n                    to_dim,\n                    to_len,\n                    from_dim,\n                    latent_dim,\n                    num_heads,\n                )\n                for _ in range(num_blocks)\n            ]\n        )\n\n\nclass TransformerTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        num_blocks,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        from_len,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            TransformerEncoderTensorDict(\n                num_blocks,\n                to_name,\n                to_name,\n                to_dim,\n                to_len,\n                to_dim,\n                latent_dim,\n                num_heads,\n            ),\n            TransformerDecoderTensorDict(\n                num_blocks,\n                from_name,\n                to_name,\n                from_dim,\n                from_len,\n                to_dim,\n                latent_dim,\n                num_heads,\n            ),\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now test our new ``TransformerTensorDict``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "to_dim = 5\nfrom_dim = 6\nlatent_dim = 10\nto_len = 3\nfrom_len = 10\nbatch_size = 8\nnum_heads = 2\nnum_blocks = 6\n\ntokens = TensorDict(\n    {\n        \"X_encode\": torch.randn(batch_size, to_len, to_dim),\n        \"X_decode\": torch.randn(batch_size, from_len, from_dim),\n    },\n    batch_size=[batch_size],\n)\n\ntransformer = TransformerTensorDict(\n    num_blocks,\n    \"X_encode\",\n    \"X_decode\",\n    to_dim,\n    to_len,\n    from_dim,\n    from_len,\n    latent_dim,\n    num_heads,\n)\n\ntransformer(tokens)\ntokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've achieved to create a transformer with ``TensorDictModule``. This\nshows that ``TensorDictModule`` is a flexible module that can implement\ncomplex operarations.\n\n### Benchmarking\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "to_dim = 5\nfrom_dim = 6\nlatent_dim = 10\nto_len = 3\nfrom_len = 10\nbatch_size = 8\nnum_heads = 2\nnum_blocks = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "td_tokens = TensorDict(\n    {\n        \"X_encode\": torch.randn(batch_size, to_len, to_dim),\n        \"X_decode\": torch.randn(batch_size, from_len, from_dim),\n    },\n    batch_size=[batch_size],\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_encode = torch.randn(batch_size, to_len, to_dim)\nX_decode = torch.randn(batch_size, from_len, from_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tdtransformer = TransformerTensorDict(\n    num_blocks,\n    \"X_encode\",\n    \"X_decode\",\n    to_dim,\n    to_len,\n    from_dim,\n    from_len,\n    latent_dim,\n    num_heads,\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n    num_blocks, to_dim, to_len, from_dim, from_len, latent_dim, num_heads\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Inference Time**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t1 = time.time()\ntokens = tdtransformer(td_tokens)\nt2 = time.time()\nprint(\"Execution time:\", t2 - t1, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t3 = time.time()\nX_out = transformer(X_encode, X_decode)\nt4 = time.time()\nprint(\"Execution time:\", t4 - t3, \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see on this minimal example that the overhead introduced by\n``TensorDictModule`` is marginal.\n\nHave fun with TensorDictModule!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}