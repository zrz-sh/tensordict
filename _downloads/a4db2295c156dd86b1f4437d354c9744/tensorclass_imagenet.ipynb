{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Batched data loading with tensorclasses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial we demonstrate how tensorclasses and memory-mapped\ntensors can be used together to efficiently and transparently load data\nfrom disk inside a model training pipeline.\n\nThe basic idea is that we pre-load the entire dataset into a\nmemory-mapped tensors, applying any non-random transformations before\nsaving to disk. This means that not only do we avoid performing repeated\ncomputation each time we iterate through the data, we also are able to\nefficiently load data from the memory-mapped tensor in batches, rather\nthan sequentially from the raw image files.\n\nUsing the combination of pre-processing, loading on a contiguous physical-memory\nstorage and on-device batched transformation, we obtain a 10x speedup in data-loading\nover regular torch + torchvision pipelines.\n\nWe\u2019ll use the same subset of imagenet used in [this transfer learning\ntutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)_,\nthough we also give results of our experiments running the same code on ImageNet.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Download the data from [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)_\n  and extract it. We assume in this tutorial that the extracted data is\n  saved in the subdirectory ``data/``.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport time\nfrom distutils.util import strtobool\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport tqdm\n\nfrom tensordict import MemmapTensor\nfrom tensordict.prototype import tensorclass\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nNUM_WORKERS = int(os.environ.get(\"NUM_WORKERS\", \"4\"))\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms\nFirst we define train and val transforms that will be applied to train and\nval examples respectively. Note that there are random components in the\ntrain transform to prevent overfitting to training data over multiple\nepochs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose(\n    [\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ]\n)\n\nval_transform = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use ``torchvision.datasets.ImageFolder`` to conveniently load and\ntransform the data from disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dir = Path(\"data\") / \"hymenoptera_data/\"\n\ntrain_data = datasets.ImageFolder(root=data_dir / \"train\", transform=train_transform)\nval_data = datasets.ImageFolder(root=data_dir / \"val\", transform=val_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We\u2019ll also create a dataset of the raw training data that simply resizes\nthe image to a common size and converts to tensor. We\u2019ll use this to\nload the data into memory-mapped tensors. The random transformations\nneed to be different each time we iterate through the data, so they\ncannot be pre-computed. We also do not scale the data yet so that we can set the\n``dtype`` of the memory-mapped array to ``uint8`` and save space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data_raw = datasets.ImageFolder(\n    root=data_dir / \"train\",\n    transform=transforms.Compose(\n        [transforms.Resize((256, 256)), transforms.PILToTensor()]\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we'll be loading our data in batches, we write a few custom transformations\nthat take advantage of this, and apply the transformations in a vectorized way.\n\nFirst a transformation that can be used for normalization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class InvAffine(nn.Module):\n    \"\"\"A custom normalization layer.\"\"\"\n\n    def __init__(self, loc, scale):\n        super().__init__()\n        self.loc = loc\n        self.scale = scale\n\n    def forward(self, x):\n        return (x - self.loc) / self.scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next two transformations that can be used to randomly crop and flip the images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class RandomHFlip(nn.Module):\n    def forward(self, x: torch.Tensor):\n        idx = (\n            torch.zeros(*x.shape[:-3], 1, 1, 1, device=x.device, dtype=torch.bool)\n            .bernoulli_()\n            .expand_as(x)\n        )\n        return x.masked_fill(idx, 0.0) + x.masked_fill(~idx, 0.0).flip(-1)\n\n\nclass RandomCrop(nn.Module):\n    def __init__(self, w, h):\n        super(RandomCrop, self).__init__()\n        self.w = w\n        self.h = h\n\n    def forward(self, x):\n        batch = x.shape[:-3]\n        index0 = torch.randint(x.shape[-2] - self.h, (*batch, 1), device=x.device)\n        index0 = index0 + torch.arange(self.h, device=x.device)\n        index0 = (\n            index0.unsqueeze(1).unsqueeze(-1).expand(*batch, 3, self.h, x.shape[-1])\n        )\n        index1 = torch.randint(x.shape[-1] - self.w, (*batch, 1), device=x.device)\n        index1 = index1 + torch.arange(self.w, device=x.device)\n        index1 = index1.unsqueeze(1).unsqueeze(-2).expand(*batch, 3, self.h, self.w)\n        return x.gather(-2, index0).gather(-1, index1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When each batch is loaded, we will scale it, then randomly crop and flip. The random\ntransformations cannot be pre-applied as they must differ each time we iterate over\nthe data. The scaling could be pre-applied in principle, but by waiting until we load\nthe data into RAM, we are able to set the dtype of the memory-mapped array to\n``uint8``, a significant space saving over ``float32``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "collate_transform = nn.Sequential(\n    InvAffine(\n        loc=torch.tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1) * 255,\n        scale=torch.tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1) * 255,\n    ),\n    RandomCrop(224, 224),\n    RandomHFlip(),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Representing data with a TensorClass\nTensorclasses are a good choice when the structure of your data is known\napriori. They are dataclasses that expose dedicated tensor methods over\ntheir contents much like a ``TensorDict``.\n\nAs well as specifying the contents (in this case ``images`` and\n``targets``) we can also encapsulate related logic as custom methods\nwhen defining the class. Here we add a classmethod that takes a dataset\nand creates a tensorclass containing the data by iterating over the\ndataset. We create memory-mapped tensors to hold the data so that they\ncan be efficiently loaded in batches later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@tensorclass\nclass ImageNetData:\n    images: torch.Tensor\n    targets: torch.Tensor\n\n    @classmethod\n    def from_dataset(cls, dataset):\n        data = cls(\n            images=MemmapTensor(\n                len(dataset),\n                *dataset[0][0].squeeze().shape,\n                dtype=torch.uint8,\n            ),\n            targets=MemmapTensor(len(dataset), dtype=torch.int64),\n            batch_size=[len(dataset)],\n        )\n        # locks the tensorclass and ensures that is_memmap will return True.\n        data.memmap_()\n\n        batch = 64\n        dl = DataLoader(dataset, batch_size=batch, num_workers=NUM_WORKERS)\n        i = 0\n        pbar = tqdm.tqdm(total=len(dataset))\n        for image, target in dl:\n            _batch = image.shape[0]\n            pbar.update(_batch)\n            data[i : i + _batch] = cls(\n                images=image, targets=target, batch_size=[_batch]\n            )\n            i += _batch\n\n        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create two tensorclasses, one for the training and on for the\nvalidation data. Note that while this step can be slightly expensive, it\nallows us to save repeated computation later during training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data_tc = ImageNetData.from_dataset(train_data_raw)\nval_data_tc = ImageNetData.from_dataset(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoaders\n\nWe can create dataloaders both from the ``torchvision``-provided\nDatasets, as well as from our memory-mapped tensorclasses.\n\nSince tensorclasses implement ``__len__`` and ``__getitem__`` (and also\n``__getitems__``) we can use them like a map-style Dataset and create a\n``DataLoader`` directly from them.\n\nSince the TensorClass data will be loaded in batches, we need to specify how these\nbatches should be collated. For this we write the following helper class\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Collate(nn.Module):\n    def __init__(self, transform=None, device=None):\n        super().__init__()\n        self.transform = transform\n        self.device = torch.device(device)\n\n    def __call__(self, x: ImageNetData):\n        # move data to RAM\n        if self.device.type == \"cuda\":\n            out = x.apply(lambda x: x.as_tensor()).pin_memory()\n        else:\n            out = x.apply(lambda x: x.as_tensor())\n        if self.device:\n            # move data to gpu\n            out = out.to(self.device)\n        if self.transform:\n            # apply transforms on gpu\n            out.images = self.transform(out.images)\n        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``DataLoader`` has support for multiple workers loading data in parallel. The\ntensorclass dataloader will use just one worker, but load data in batches.\n\nNote that under this approach our ``collate_fn`` is essentially just an ``nn.Module``,\nmaking it transparent and easy to implement. But this approach also offers\nflexibility, for example, if needed we could move the collation step into the training\nloop by considering the ``Collate`` module as part of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 8\ntrain_dataloader = DataLoader(\n    train_data,\n    batch_size=batch_size,\n    num_workers=NUM_WORKERS,\n)\nval_dataloader = DataLoader(\n    val_data,\n    batch_size=batch_size,\n    num_workers=NUM_WORKERS,\n)\n\ntrain_dataloader_tc = DataLoader(\n    train_data_tc,\n    batch_size=batch_size,\n    collate_fn=Collate(collate_transform, device),\n)\nval_dataloader_tc = DataLoader(\n    val_data_tc,\n    batch_size=batch_size,\n    collate_fn=Collate(device=device),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compare how long it takes to iterate once over the data in\neach case. The regular dataloader loads images one by one from disk,\napplies the transform sequentially and then stacks the results\n(note: we start measuring time a little after the first iteration, as\nstarting the dataloader can take some time).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total = 0\nfor i, (image, target) in enumerate(train_dataloader):\n    if i == 3:\n        t0 = time.time()\n    if i >= 3:\n        total += image.shape[0]\n    image, target = image.to(device), target.to(device)\nt = time.time() - t0\nprint(f\"One iteration over dataloader done! Rate: {total/t:4.4f} fps, time: {t: 4.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our tensorclass-based dataloader instead loads data from the\nmemory-mapped tensor in batches. We then apply the batched random\ntransformations to the batched images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total = 0\nfor i, batch in enumerate(train_dataloader_tc):\n    if i == 3:\n        t0 = time.time()\n    if i >= 3:\n        total += batch.numel()\n    image, target = batch.images, batch.targets\nt = time.time() - t0\nprint(\n    f\"One iteration over tensorclass dataloader done! Rate: {total/t:4.4f} fps, time: {t: 4.4f}s\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the case of the validation set, we see an even bigger performance\nimprovement, because there are no random transformations, so we can save\nthe fully transformed data in the memory-mapped tensor, eliminating the\nneed for additional transformations as we load from disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total = 0\nfor i, (image, target) in enumerate(val_dataloader):\n    if i == 3:\n        t0 = time.time()\n    if i >= 3:\n        total += image.shape[0]\n    image, target = image.to(device), target.to(device)\nt = time.time() - t0\nprint(f\"One iteration over val data done! Rate: {total/t:4.4f} fps, time: {t: 4.4f}s\")\n\ntotal = 0\nfor i, batch in enumerate(val_dataloader_tc):\n    if i == 3:\n        t0 = time.time()\n    if i >= 3:\n        total += batch.shape[0]\n    image, target = batch.images.contiguous().to(device), batch.targets.contiguous().to(\n        device\n    )\nt = time.time() - t0\nprint(\n    f\"One iteration over tensorclass val data done! Rate: {total/t:4.4f} fps, time: {t: 4.4f}s\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results from ImageNet\n\nWe repeated the above on full-size ImageNet data, running on an AWS EC2 instance with\n32 cores and 1 A100 GPU. We compare against the regular ``DataLoader`` with different\nnumbers of workers. We found that our single-threaded TensorClass approach\nout-performed the ``DataLoader`` even when we used a large number of workers.\n\n<img src=\"reference/generated/tutorials/media/reference/generated/tutorials/media/imagenet-benchmark-time.png\" alt=\"Bar chart showing runtimes of dataloaders compared with TensorClass\">\n\n<img src=\"reference/generated/tutorials/media/reference/generated/tutorials/media/imagenet-benchmark-speed.png\" alt=\"Bar chart showing collection rate of dataloaders compared with TensorClass\">\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows that much of the overhead is coming from i/o operations rather than the\ntransforms, and hence explains how the memory-mapped array helps us load data more\nefficiently. Check out the [distributed example](https://github.com/pytorch-labs/tensordict/tree/main/benchmarks/distributed/dataloading.py)_\nfor more context about the other results from these charts.\n\nWe can get even better performance with the TensorClass approach by using multiple\nworkers to load batches from the memory-mapped array, though this comes with some\nadded complexity. See [this example in our benchmarks](https://github.com/pytorch-labs/tensordict/blob/main/benchmarks/distributed/dataloading.py)_\nfor an example of how this could work.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}